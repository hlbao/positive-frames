colab link: 
https://colab.research.google.com/drive/1B3-_WR_ge9Teea6tGzF-E5Hk44wCX_wk?usp=sharing

note that we currenly focus on two deep learning models: [Sbert](https://www.sbert.net/) and   [BART (with no constraints)](https://ai.facebook.com/research/publications/bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/)
